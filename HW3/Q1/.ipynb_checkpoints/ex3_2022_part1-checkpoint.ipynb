{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NwLQOFMMlev"
   },
   "source": [
    "# Computer Vision: Assignment 3 Part 1 - Some CNN Basics [50%]\n",
    "\n",
    "Spring 2022 semester.\n",
    "\n",
    "Due date: **July 1st 2022.**\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This question is an introduction to using PyTorch for training simple neural net models. \n",
    "\n",
    "Two different datasets will be used: \n",
    "- MNIST digits [handwritten digits]\n",
    "- CIFAR-10 [32x32 resolution color images of 10 object classes].\n",
    "\n",
    "## Requirements\n",
    "\n",
    "Perform this assignment in PyTorch, modifying the baseline code, producing outputs and adding required explanations - *all within this ipython notebook*.\n",
    "\n",
    "Finally, submit this iPython notebook, including outputs, as an .ipynb file. \n",
    "\n",
    "## Question (1): Warmup [5%]\n",
    "\n",
    "It is always good practice to visually inspect your data before trying to train a model, since it lets you check for problems and get a feel for the task at hand.\n",
    "\n",
    "MNIST is a dataset of 70,000 grayscale hand-written digits (0 through 9).\n",
    "60,000 of these are training images. 10,000 are a held out test set. \n",
    "\n",
    "CIFAR-10 is a dataset of 60,000 color images (32 by 32 resolution) across 10 classes (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck). \n",
    "The train/test split is 50k/10k.\n",
    "\n",
    "**a)**  Use `matplotlib` and ipython notebook's visualization capabilities to display the average train and average test image of each class, for each of the two datasets.\n",
    "[See this PyTorch tutorial page](http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) for hints on how to achieve this.\n",
    "\n",
    "## Question (2): Training a Single Layer Network on MNIST [10%]\n",
    "\n",
    "Start by running the training on MNIST. By default if you run this notebook successfully, it will train on MNIST.\n",
    "\n",
    "This will initialize a single layer model train it on the 60,000 MNIST training images for 10 epochs (passes through the training data). \n",
    "\n",
    "The cross-entropy loss function computes a Logarithm of the Softmax on the output of the neural network, and then computes the negative log-likelihood w.r.t. the given target.\n",
    "\n",
    "The default values for the learning rate, batch size and number of epochs are given in the \"options\" cell of this notebook. Unless otherwise specified, use the default values throughout this assignment. \n",
    "\n",
    "Note the decrease in training loss and corresponding decrease in validation errors.\n",
    "\n",
    "**a)** Add code to plot out the network weights as images (one for each class, of size 28 by 28) after the last epoch. (Hint thread: [#1](https://discuss.pytorch.org/t/understanding-deep-network-visualize-weights/2060/2?u=smth) )\n",
    "\n",
    "**b)** Reduce the number of training examples to just 50. [Hint: limit the iterator in the `train` function]. \n",
    "Paste the output into your report and explain what is happening to the model.\n",
    "\n",
    "## Question (3): Training a Multi-Layer Network on MNIST [10%]\n",
    "\n",
    "**a)**  Add an extra layer to the network with 1000 hidden units and a `tanh` non-linearity. [Hint: modify the `Net` class].\n",
    "\n",
    "**b)**   Now retrain the model for 10 epochs with each of the learning rates in the set {0.01, 0.1, 1, 10} and test the resulting model. Create a figure and plot the loss curves of each of the four runs for comparison. Explain the obtained (train and test) results.\n",
    "\n",
    "## Question (4): Training a Convolutional Network on CIFAR [15%]\n",
    "\n",
    "To change over to the CIFAR-10 dataset, change the `options` cell's `dataset` variable to `'cifar10'`.\n",
    "\n",
    "- Create a convolutional network with the following architecture:\n",
    "  - Convolution with 5 by 5 filters, stride 2, 16 feature maps + Tanh nonlinearity.\n",
    "  - Convolution with 3 by 3 filters, (stride 1), 64 feature maps + Tanh nonlinearity.\n",
    "  - Convolution with 3 by 3 filters, (stride 1), 64 feature maps + Tanh nonlinearity.\n",
    "  - 2 by 2 max pooling (non-overlapping).\n",
    "  - Flatten to vector.\n",
    "  - Linear layer with 64 hidden units + Tanh nonlinearity.\n",
    "  - Linear layer to 10 output units.\n",
    "\n",
    "**a)** Train it for 20 epochs on CIFAR-10 and plot the training loss curve (avg. loss per epoch) and final test loss and accuracy, when training with or without the two 3x3 convolutional layers. \n",
    "\n",
    "**b)**  Give a breakdown of the number of parameters (per layer) within the above model, as well as the overall number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TguAK3vIgJ_"
   },
   "source": [
    "# **Answer (1):**\n",
    "create needed blocks of code (and results) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img1, img2, dataset, label):\n",
    "    img1 = img1 / 2 + 0.5  # un-normalize\n",
    "    img2 = img2 / 2 + 0.5\n",
    "    np_img1, np_img2 = img1.numpy(), img2.numpy()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.suptitle(dataset + \" dataset average: Class \" + label)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(np.transpose(np_img1, (1, 2, 0)))\n",
    "    plt.title('Train')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(np.transpose(np_img2, (1, 2, 0)))\n",
    "    plt.title('Test')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_average_image(train_loader, test_loader, dataset, labels):\n",
    "    \n",
    "    train_counters, test_counters = [], []\n",
    "    train_results, test_results = [], []\n",
    "    \n",
    "    for i in range(len(labels)):\n",
    "        train_counters.append(0)\n",
    "        test_counters.append(0)\n",
    "        train_results.append(None)\n",
    "        test_results.append(None)\n",
    "    \n",
    "    dataiter = iter(train_loader)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            img_temp, label_temp = dataiter.next()\n",
    "            \n",
    "            for ind in range(len(labels)):\n",
    "                print(temp_label)\n",
    "                break\n",
    "                if label_temp == 'tensor[(' + str(ind) + ')]':\n",
    "                    train_counters[ind] += 1\n",
    "                    if train_results[ind] is None:\n",
    "                        train_results[ind] = img_temp\n",
    "                    else:\n",
    "                        train_results[ind] += img_temp\n",
    "\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "    dataiter = iter(test_loader)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            img_temp, label_temp = dataiter.next()\n",
    "            \n",
    "            for ind in range(len(labels)):\n",
    "                print(temp_label)\n",
    "                break\n",
    "                if label_temp == 'tensor[(' + str(ind) + ')]':\n",
    "                    test_counters[ind] += 1\n",
    "                    if test_results[ind] is None:\n",
    "                        test_results[ind] = img_temp\n",
    "                    else:\n",
    "                        test_results[ind] += img_temp\n",
    "\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "    for index, label in enumerate(labels):\n",
    "        \n",
    "        test_results[index] /= test_counters[index]\n",
    "        train_results[index] /= train_counters[index]\n",
    "\n",
    "        imshow(torchvision.utils.make_grid(train_results[index]), torchvision.utils.make_grid(test_results[index]), \n",
    "               dataset, labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /=: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2312/1704971609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mmnist_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'0'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'3'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'4'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'5'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'6'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'7'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'9'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[0mshow_average_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcifar_train_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar_test_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'CIFAR-10'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcifar_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[0mshow_average_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnist_train_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_test_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MNIST'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmnist_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2312/2244327831.py\u001b[0m in \u001b[0;36mshow_average_image\u001b[1;34m(train_loader, test_loader, dataset, labels)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0mtest_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtest_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m         \u001b[0mtrain_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mtrain_counters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /=: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "torch.utils.data.DataLoader.num_worker = 0\n",
    "\n",
    "cifar_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "mnist_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "cifar_train = torchvision.datasets.CIFAR10(root='.', train=True,\n",
    "                                           download=True, transform=cifar_transform)\n",
    "\n",
    "cifar_train_loader = torch.utils.data.DataLoader(cifar_train, batch_size=1,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "\n",
    "cifar_test = torchvision.datasets.CIFAR10(root='.', train=False,\n",
    "                                          download=True, transform=cifar_transform)\n",
    "\n",
    "cifar_test_loader = torch.utils.data.DataLoader(cifar_test, batch_size=1,\n",
    "                                                shuffle=False, num_workers=2)\n",
    "\n",
    "mnist_train = torchvision.datasets.MNIST(root='.', train=True,\n",
    "                                         download=True, transform=mnist_transform)\n",
    "\n",
    "mnist_train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=1,\n",
    "                                                 shuffle=True, num_workers=2)\n",
    "\n",
    "mnist_test = torchvision.datasets.MNIST(root='.', train=False,\n",
    "                                        download=True, transform=mnist_transform)\n",
    "\n",
    "mnist_test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=1,\n",
    "                                                shuffle=False, num_workers=2)\n",
    "\n",
    "cifar_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "mnist_labels = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "show_average_image(cifar_train_loader, cifar_test_loader, 'CIFAR-10', cifar_labels)\n",
    "show_average_image(mnist_train_loader, mnist_test_loader, 'MNIST', mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feeFGXsqI8hP"
   },
   "source": [
    "# **Answer (2):**\n",
    "create needed blocks of code (and results) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "dataset = 'mnist' # options: 'mnist' | 'cifar10'\n",
    "batch_size = 64   # input batch size for training\n",
    "epochs = 10       # number of epochs to train\n",
    "lr = 0.01        # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## network and optimizer\n",
    "if dataset == 'mnist':\n",
    "    num_inputs = 784\n",
    "elif dataset == 'cifar10':\n",
    "    num_inputs = 3072\n",
    "\n",
    "num_outputs = 10 # same for both CIFAR10 and MNIST, both have 10 classes as outputs\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_inputs, num_outputs):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear = nn.Linear(num_inputs, num_outputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, num_inputs) # reshape input to batch x num_inputs\n",
    "        output = self.linear(input)\n",
    "        return output\n",
    "\n",
    "network = Net(num_inputs, num_outputs)\n",
    "optimizer = optim.SGD(network.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train and test functions\n",
    "def train(epoch, train_loader):\n",
    "    network.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = network(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(test_loader):\n",
    "    network.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        #data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = network(data)\n",
    "        test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n",
    "        #test_loss += F.cross_entropy(output, target, sum=True).item() # sum up batch loss\n",
    "        pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and testing \n",
    "for i in range(1, epochs+1):\n",
    "  train(i, mnist_train_loader)\n",
    "\n",
    "test(mnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "en_GCPLkQ_00"
   },
   "source": [
    "# **Answer (3):**\n",
    "create needed blocks of code (and results) below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dk4Q_EDORBX2"
   },
   "source": [
    "# **Answer (4):**\n",
    "create needed blocks of code (and results) below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5NwLQOFMMlev"
   ],
   "name": "ex3_2022_part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
