{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K29I-OwCEYzW",
    "outputId": "db8b297c-a4b8-457a-b43f-7570135306c7"
   },
   "source": [
    "# Computer Vision: Assignment 3 Part 2 - Image De-blurring by Supervised Learning [50%]\n",
    "\n",
    "Spring 2022 semester.\n",
    "\n",
    "Due date: **July 1st 2022.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p77qywzzEnE6"
   },
   "source": [
    "# GUIDELINES\n",
    "\n",
    "## 1. Problem Definition\n",
    "\n",
    "### (1) Data\n",
    "\n",
    "- input data consists of `training` data and `testing` data\n",
    "- `training` data is used for training neural network\n",
    "- `testing` data is used for validating the trained neural network\n",
    "- both `training` and `testing` data have the same structure that consists of a pair of clean image 'x' and its corrupted blurry image 'y'\n",
    "- images are 2-dimensional matrices of the size `256 x 256` (gray-scale images)\n",
    "\n",
    "### (2) Neural Network\n",
    "\n",
    "- A simple neural network is provided, in the form of auto-encoder that consists of an encoder and a decoder\n",
    "- a typical encoder consists of layers including convolution, pooling/striding, batch normalization, activation\n",
    "- a typical decoder consists of layers including upsampling, convolution, batch normalization, activation\n",
    "- Sigmoid is a typical final activation function for the $[0,1]$-scaled network output\n",
    "  \n",
    "### (3) Loss\n",
    "\n",
    "- The network uses the mean squared error (the squared `L_2`-norm) between the prediction (de-blurred) and the ground truth (original)\n",
    "\n",
    "### (4) Optimization\n",
    "\n",
    "- use any optimization algorithms such as SGD, Adam, AdaGrad, RMSProp\n",
    "\n",
    "### (5) Training\n",
    "\n",
    "- training aims to determine the model parameters of the neural network and its associated loss function is optimized using the training data\n",
    "\n",
    "### (6) Testing\n",
    "\n",
    "- testing aims to validate the generality of the trained neural network using the testing data\n",
    "\n",
    "### (7) Evaluation metrics\n",
    "\n",
    "- use PSNR and SSIM for the evaluation of the performance\n",
    "- PSNR is computed by `PSNR = 10 * \\log_{10} \\left( \\frac{MAX(IMAGE)^2}{MSE} \\right)`\n",
    "- SSIM (structural similarity index) is following  Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: From error visibility to structural similarity. https://ece.uwaterloo.ca/~z70wang/publications/ssim.pdf\n",
    "\n",
    "\n",
    "\n",
    "## 3. General goal: \n",
    "\n",
    "Work on the model to get the best performance (PSNR, SSIM) on the testing data. You may play around, for example , with the following(but not necessarily with all!):\n",
    "\n",
    "- neural network architecture\n",
    "- optimizer\n",
    "- loss function\n",
    "- data augmentation\n",
    "- initialization\n",
    "- number of epochs\n",
    "- size of mini-batch\n",
    "- learning-rate\n",
    "- weight-decay\n",
    "- early stopping\n",
    "- ..\n",
    "  \n",
    "## 4. Submission\n",
    "\n",
    "### (1) this notebook file (with cell output results)\n",
    "### (2) the file model.pth (saved below) which holds the weights of your final model\n",
    "\n",
    "## 5. Grading\n",
    "\n",
    "Will be based on:\n",
    "\n",
    "- accuracy (PSNR, SSIM) performance on a held-out test set that we will check\n",
    "- explanation (last text block) on what were the architectural and other choices that you made \n",
    "- answer of last 'question' at the bottom of the file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3TClD-gXSKwu"
   },
   "source": [
    "## install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2keoUZI7lDss"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'wget' from 'C:\\\\Users\\\\Karee\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\wget.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You may add here anything needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmR8_EyrCMfU"
   },
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "zN9lRHk6CMfW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from math import log10\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import os\n",
    "from skimage.metrics import structural_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGDP7gNBmA1y"
   },
   "source": [
    "## download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "rvjHgZ6SlNBi",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The syntax of the command is incorrect.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "filename_data   = './data/ex3_P2_data_public.npz'\n",
    "if os.path.exists(filename_data):\n",
    "  print('data already exists')\n",
    "else:\n",
    "  print('downloading data...')\n",
    "  !mkdir './data'\n",
    "  !wget -O './data/ex3_P2_data_public.npz' https://www.cs.haifa.ac.il/~skorman/ex3_P2_data_public.npz\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrWykA_0CMfc"
   },
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "38nFrQ7-CMfe"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/ex3_P2_data_public.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23732/4211796280.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfilename_data\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[1;34m'./data/ex3_P2_data_public.npz'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m            \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moriginal_train\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'original_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mblur_train\u001b[0m      \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'blur_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[0;32m    405\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/ex3_P2_data_public.npz'"
     ]
    }
   ],
   "source": [
    "filename_data   = './data/ex3_P2_data_public.npz'\n",
    "data            = np.load(filename_data)\n",
    "\n",
    "original_train  = data['original_train']\n",
    "blur_train      = data['blur_train']\n",
    "\n",
    "original_test   = data['original_test']\n",
    "blur_test       = data['blur_test']\n",
    "\n",
    "num_data_train  = original_train.shape[0]\n",
    "num_data_test   = original_test.shape[0]\n",
    "\n",
    "\n",
    "print('*************************************************')\n",
    "print('size of original_train :', original_train.shape)\n",
    "print('size of blur_train :', blur_train.shape)\n",
    "print('*************************************************')\n",
    "print('size of original_test :', original_test.shape)\n",
    "print('size of blur_test :', blur_test.shape)\n",
    "print('*************************************************')\n",
    "print('number of training image :', original_train.shape[0])\n",
    "print('height of training image :', original_train.shape[1])\n",
    "print('width of training image :', original_train.shape[2])\n",
    "print('*************************************************')\n",
    "print('number of testing image :', original_test.shape[0])\n",
    "print('height of testing image :', original_test.shape[1])\n",
    "print('width of testing image :', original_test.shape[2])\n",
    "print('*************************************************')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeulKYLYCMfj"
   },
   "source": [
    "## hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zhUDgSYGCMfm"
   },
   "outputs": [],
   "source": [
    "device          = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "number_epoch    = 100\n",
    "size_minibatch  = 50\n",
    "learning_rate   = 0.1\n",
    "weight_decay    = 0.0000001\n",
    "\n",
    "# any others?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOgi3wV2CMfq"
   },
   "source": [
    "## data loader (including augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v2Vb2dEVCMfs"
   },
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    \n",
    "    def __init__(self, original, blur, transform=False):\n",
    "        \n",
    "        self.original   = original\n",
    "        self.blur       = blur \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        original    = self.original[index]\n",
    "        blur        = self.blur[index]\n",
    "        \n",
    "        original    = torch.FloatTensor(original).unsqueeze(dim=0)\n",
    "        blur        = torch.FloatTensor(blur).unsqueeze(dim=0)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            crop_size = [64, 64]\n",
    "            # random crop\n",
    "            top         = random.randint(0, original.shape[1] - crop_size[0])\n",
    "            left        = random.randint(0, original.shape[2] - crop_size[1])\n",
    "            original    = transforms.functional.crop(original, top, left, crop_size[0], crop_size[1])\n",
    "            blur        = transforms.functional.crop(blur, top, left, crop_size[0], crop_size[1])\n",
    "            \n",
    "            # anything else?\n",
    "\n",
    "        return (original, blur)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.original.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3xmXKzyCMfw"
   },
   "source": [
    "## construct datasets and dataloaders for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgtodBbwCMfy"
   },
   "outputs": [],
   "source": [
    "dataset_train_transform = dataset(original_train, blur_train, transform=True)\n",
    "dataset_train           = dataset(original_train, blur_train)\n",
    "dataset_test            = dataset(original_test, blur_test)\n",
    "\n",
    "dataloader_train_transform  = DataLoader(dataset_train_transform, batch_size=size_minibatch, shuffle=True, drop_last=True)\n",
    "dataloader_train            = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=False, drop_last=True)\n",
    "dataloader_test             = DataLoader(dataset_test, batch_size=size_minibatch, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndwzntIxm82b"
   },
   "source": [
    "## helper function for visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgtYot10nQ17"
   },
   "outputs": [],
   "source": [
    "# visualize a sample of the data\n",
    "def plot_data_grid(data, index_data, nRow, nCol):\n",
    "    \n",
    "    size_col = 2\n",
    "    size_row = 2\n",
    "\n",
    "    fig, axes = plt.subplots(nRow, nCol, constrained_layout=True, figsize=(nCol * size_col, nRow * size_row))\n",
    "\n",
    "    data = data.detach().cpu().squeeze(axis=1)\n",
    "\n",
    "    if nRow==1:\n",
    "        for j in range(nCol):\n",
    "            index   = index_data[j]\n",
    "\n",
    "            axes[j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
    "            axes[j].xaxis.set_visible(False)\n",
    "            axes[j].yaxis.set_visible(False)\n",
    "    else:        \n",
    "        for i in range(nRow):\n",
    "            for j in range(nCol):\n",
    "                k       = i * nCol + j\n",
    "                index   = index_data[k]\n",
    "\n",
    "                axes[i, j].imshow(data[index], cmap='gray', vmin=0, vmax=1)\n",
    "                axes[i, j].xaxis.set_visible(False)\n",
    "                axes[i, j].yaxis.set_visible(False)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6QR-wM8QE7S"
   },
   "source": [
    "## visualize your (augmented) training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOI4YIhIQMcB"
   },
   "outputs": [],
   "source": [
    "nRow = 1\n",
    "nCol = 10\n",
    "index_data = np.arange(nRow * nCol) # show only first images\n",
    "\n",
    "for index_batch, (original, blur) in enumerate(dataloader_train_transform):\n",
    "    print('sample from augmented batch', index_batch, '  image dims', original[0].size())\n",
    "    plot_data_grid(original, index_data, nRow, nCol)     \n",
    "    plot_data_grid(blur, index_data, nRow, nCol)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZn19cHQCMf0"
   },
   "source": [
    "## shape of the data when using the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qACVn6ogCMf1"
   },
   "outputs": [],
   "source": [
    "(original_train, blur_train)  = dataset_train[0]\n",
    "(original_test, blur_test)    = dataset_test[0]\n",
    "(original_train_transform, blur_train_transform)  = dataset_train_transform[0]\n",
    "print('*******************************************************************')\n",
    "print('shape of the original in the training dataset:', original_train.shape)\n",
    "print('shape of the blur in the training dataset:', blur_train.shape)\n",
    "print('*******************************************************************')\n",
    "print('shape of the original in the testing dataset:', original_test.shape)\n",
    "print('shape of the blur in the testing dataset:', blur_test.shape)\n",
    "print('*******************************************************************')\n",
    "print('shape of the original in the training transform dataset:', original_train_transform.shape)\n",
    "print('shape of the blur in the training transform dataset:', blur_train_transform.shape)\n",
    "print('*******************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMrs2Oy9CMf3"
   },
   "source": [
    "## neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLvysjDSCMf5"
   },
   "outputs": [],
   "source": [
    "class Network(nn.Module): \n",
    "\n",
    "\tdef __init__(self, in_channel=1, out_channel=1, dim_feature=8, threshold_ReLU=0.02):\n",
    "        \n",
    "\t\tsuper(Network, self).__init__()\n",
    "\t\n",
    "\t\t# *********************************************************************\n",
    "\t\t# define your layer types here\n",
    "\t\t# *********************************************************************\n",
    "\n",
    "\t\t# 'encoder' layers\n",
    "\t\tself.in_channel \t= in_channel\n",
    "\t\tself.out_channel\t= out_channel\n",
    "\t\tself.dim_feature\t= dim_feature\n",
    "\t\t\n",
    "\t\tself.conv_encode1\t= nn.Conv2d(in_channel , dim_feature * 1, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "\t\tself.conv_encode2\t= nn.Conv2d(dim_feature * 1, dim_feature * 2, kernel_size=3, stride=2, padding=1, bias=True)\n",
    "\n",
    "\t\t# 'decoder' layers\n",
    "\t\tself.conv_decode2 \t= nn.Conv2d(dim_feature * 2, dim_feature * 1, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\t\tself.conv_decode1 \t= nn.Conv2d(dim_feature * 1, dim_feature * 1, kernel_size=3, stride=1, padding=1, bias=True)\n",
    "\t\tself.conv_out \t\t= nn.Conv2d(dim_feature * 1, out_channel,\tkernel_size=1, stride=1, padding=0, bias=True)\n",
    "\n",
    "\t\t# batchnorm layers\n",
    "\t\tself.ebn1\t\t\t= nn.BatchNorm2d(dim_feature * 1)\n",
    "\t\tself.ebn2\t\t\t= nn.BatchNorm2d(dim_feature * 2)\n",
    "\t\tself.dbn2\t\t\t= nn.BatchNorm2d(dim_feature * 1)\n",
    "\t\tself.dbn1\t\t\t= nn.BatchNorm2d(dim_feature * 1)\n",
    "\n",
    "\t\t# activation layers\n",
    "\t\tself.activation\t\t= nn.LeakyReLU(threshold_ReLU, inplace=True)\n",
    "\t\tself.activation_out\t= nn.Sigmoid()\n",
    "\n",
    "\n",
    "\t\t# *********************************************************************\n",
    "\t\t# forward propagation - build your network here ('forward' is called when network is applied)\n",
    "\t\t# *********************************************************************\n",
    "\tdef forward(self, x):\n",
    "\n",
    "\t\t# 'encoder' layer 1\n",
    "\t\tx1  = self.conv_encode1(x)\n",
    "\t\teb1 = self.ebn1(x1)\n",
    "\t\te1  = self.activation(eb1)\n",
    "\n",
    "\t\t# # possible 'encoder' layer 2\n",
    "\t\t# x2  = self.conv_encode2(e1)\n",
    "\t\t# eb2 = self.ebn2(x2)\n",
    "\t\t# e2  = self.activation(eb2)\n",
    "\t\t\t\t\n",
    "\t\t# # possible 'decoder' layer 2\n",
    "\t\t# y2  = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)(e2)\n",
    "\t\t# y2  = self.conv_decode2(y2) \n",
    "\t\t# db2 = self.dbn2(y2)\n",
    "\t\t# d2  = self.activation(db2)\n",
    "\t\n",
    "\t\t# 'decoder' layer 1\n",
    "\t\ty1  = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=False)(eb1)\n",
    "\t\ty1  = self.conv_decode1(y1) \n",
    "\t\tdb1 = self.dbn1(y1)\n",
    "\t\td1  = self.activation(db1)\t \n",
    "\t\t\n",
    "\t\ty1  = self.conv_out(d1)\n",
    "\t\ty = self.activation_out(y1)\n",
    "\n",
    "\t\treturn y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWvc1MXUCMf7"
   },
   "source": [
    "## build network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YS6qr0zMCMf9"
   },
   "outputs": [],
   "source": [
    "model       = Network().to(device)\n",
    "optimizer   = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "# try other optimizers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fre5Vv6jCMgA"
   },
   "source": [
    "## helper functions: accuracy (PSNR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5CRefun9CMgB"
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(prediction, label):\n",
    "\n",
    "    # # ssim\n",
    "\n",
    "    # def eval_step(engine, batch):\n",
    "    #     return batch\n",
    "\n",
    "    # default_evaluator = Engine(eval_step)\n",
    "    # metric = SSIM(data_range=1.0)\n",
    "    # metric.attach(default_evaluator, 'ssim')\n",
    "    # state = default_evaluator.run([[prediction, label]])\n",
    "    # ssim = state.metrics['ssim']\n",
    "\n",
    "    preds = prediction.squeeze().detach().numpy()\n",
    "    targets = label.squeeze().detach().numpy()\n",
    "    ssims = np.zeros(preds.shape[0])\n",
    "    for i in range(preds.shape[0]):\n",
    "        ssims[i] = structural_similarity(preds[i], targets[i], data_range=1)\n",
    "    \n",
    "    ssim = ssims.mean()\n",
    "\n",
    "    # psnr\n",
    "\n",
    "    prediction  = prediction.squeeze(axis=1)\n",
    "    label       = label.squeeze(axis=1)\n",
    "    mse_loss    = torch.mean((prediction - label) ** 2)\n",
    "\n",
    "    if mse_loss == 0.0:\n",
    "        psnr = 100\n",
    "    else:\n",
    "        psnr = 10 * torch.log10(1 / mse_loss)\n",
    "\n",
    "    psnr = psnr.item()\n",
    "    \n",
    "    return psnr, ssim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YbNbRlerCMgE"
   },
   "source": [
    "## variables for the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66pKtme9CMgF"
   },
   "outputs": [],
   "source": [
    "loss_mean_train   = np.zeros(number_epoch)\n",
    "loss_std_train    = np.zeros(number_epoch)\n",
    "psnr_mean_train   = np.zeros(number_epoch)\n",
    "psnr_std_train    = np.zeros(number_epoch)\n",
    "ssim_mean_train   = np.zeros(number_epoch)\n",
    "ssim_std_train    = np.zeros(number_epoch)\n",
    "\n",
    "loss_mean_test    = np.zeros(number_epoch)\n",
    "loss_std_test     = np.zeros(number_epoch)\n",
    "psnr_mean_test    = np.zeros(number_epoch)\n",
    "psnr_std_test     = np.zeros(number_epoch)\n",
    "ssim_mean_test    = np.zeros(number_epoch)\n",
    "ssim_std_test     = np.zeros(number_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X2-nbFIgCMgG"
   },
   "source": [
    "## train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vu1jQ-OcCMgG"
   },
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "\n",
    "    loss_epoch = []\n",
    "    psnr_epoch = []\n",
    "    ssim_epoch = []\n",
    "    loss_criterion   = nn.MSELoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for index_batch, (original, blur) in enumerate(dataloader):\n",
    "\n",
    "        original    = original.to(device)\n",
    "        blur        = blur.to(device)\n",
    "        \n",
    "        # prediction\n",
    "        prediction  = model(blur)\n",
    "\n",
    "        # loss\n",
    "        loss        = loss_criterion(prediction, original)\n",
    "\n",
    "        # accuracy\n",
    "        psnr, ssim    = compute_accuracy(prediction, original)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_epoch.append(loss.item())\n",
    "        psnr_epoch.append(psnr)\n",
    "        ssim_epoch.append(ssim)\n",
    "\n",
    "    loss_mean_epoch     = np.mean(loss_epoch)\n",
    "    loss_std_epoch      = np.std(loss_epoch)\n",
    "\n",
    "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
    "    psnr_std_epoch  = np.std(psnr_epoch)\n",
    "    ssim_mean_epoch = np.mean(ssim_epoch)\n",
    "    ssim_std_epoch  = np.std(ssim_epoch)\n",
    "\n",
    "    loss    = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
    "    psnr    = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
    "    ssim    = {'mean' : ssim_mean_epoch, 'std' : ssim_std_epoch}\n",
    "\n",
    "    return (loss, psnr, ssim)   \n",
    "\n",
    "\n",
    "def test(model, dataloader):\n",
    "\n",
    "    loss_epoch = []\n",
    "    psnr_epoch = []\n",
    "    ssim_epoch = []\n",
    "    loss_criterion   = nn.MSELoss()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for index_batch, (original, blur) in enumerate(dataloader):\n",
    "\n",
    "        original    = original.to(device)\n",
    "        blur        = blur.to(device)\n",
    "\n",
    "        # prediction\n",
    "        prediction  = model(blur)\n",
    "\n",
    "        # loss\n",
    "        loss        = loss_criterion(prediction, original)        \n",
    "        \n",
    "        # accuracy\n",
    "        psnr, ssim    = compute_accuracy(prediction, original)\n",
    "\n",
    "        loss_epoch.append(loss.item())\n",
    "        psnr_epoch.append(psnr)\n",
    "        ssim_epoch.append(ssim)\n",
    "\n",
    "    loss_mean_epoch     = np.mean(loss_epoch)\n",
    "    loss_std_epoch      = np.std(loss_epoch)\n",
    "\n",
    "    psnr_mean_epoch = np.mean(psnr_epoch)\n",
    "    psnr_std_epoch  = np.std(psnr_epoch)\n",
    "    ssim_mean_epoch = np.mean(ssim_epoch)\n",
    "    ssim_std_epoch  = np.std(ssim_epoch)\n",
    "\n",
    "    loss    = {'mean' : loss_mean_epoch, 'std' : loss_std_epoch}\n",
    "    psnr    = {'mean' : psnr_mean_epoch, 'std' : psnr_std_epoch}\n",
    "    ssim    = {'mean' : ssim_mean_epoch, 'std' : ssim_std_epoch}\n",
    "\n",
    "    return (loss, psnr, ssim)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5nmz3mMCMgK"
   },
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbImpvcnCMgK"
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# \n",
    "# iterations for epochs\n",
    "#\n",
    "# ================================================================================\n",
    "for i in tqdm(range(number_epoch)):\n",
    "    \n",
    "    # ================================================================================\n",
    "    # \n",
    "    # training\n",
    "    #\n",
    "    # ================================================================================\n",
    "    (loss_train, psnr_train, ssim_train) = train(model, dataloader_train_transform)\n",
    "\n",
    "    loss_mean_train[i]      = loss_train['mean']\n",
    "    loss_std_train[i]       = loss_train['std']\n",
    "\n",
    "    psnr_mean_train[i]  = psnr_train['mean']\n",
    "    psnr_std_train[i]   = psnr_train['std']\n",
    "\n",
    "    ssim_mean_train[i]  = ssim_train['mean']\n",
    "    ssim_std_train[i]   = ssim_train['std']\n",
    "\n",
    "    # ================================================================================\n",
    "    # \n",
    "    # testing (validation)\n",
    "    #\n",
    "    # ================================================================================\n",
    "    (loss_test, psnr_test, ssim_test) = test(model, dataloader_test)\n",
    "\n",
    "    loss_mean_test[i]      = loss_test['mean']\n",
    "    loss_std_test[i]       = loss_test['std']\n",
    "\n",
    "    psnr_mean_test[i]  = psnr_test['mean']\n",
    "    psnr_std_test[i]   = psnr_test['std']\n",
    "\n",
    "    ssim_mean_test[i]  = ssim_test['mean']\n",
    "    ssim_std_test[i]   = ssim_test['std']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n4oZMR4XCMgM"
   },
   "source": [
    "# functions for visualizing the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaPKLOggCMgO"
   },
   "source": [
    "## plot curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0uW5bLYbCMgP"
   },
   "outputs": [],
   "source": [
    "# plot train and test metric along epochs\n",
    "def plot_curve_error(train_mean, train_std, test_mean, test_std, x_label, y_label, title, identity=[]):\n",
    "\n",
    "    plt.figure(figsize=(10,8))\n",
    "    plt.title(title)\n",
    "\n",
    "    alpha = 0.1\n",
    "    \n",
    "    plt.plot(range(len(train_mean)), train_mean, '-', color = 'red', label='train')\n",
    "    plt.fill_between(range(len(train_mean)), train_mean - train_std, train_mean + train_std, facecolor = 'red', alpha = alpha) \n",
    "\n",
    "    plt.plot(range(len(test_mean)), test_mean, '-', color = 'blue', label='test')\n",
    "    plt.fill_between(range(len(test_mean)), test_mean - test_std, test_mean + test_std, facecolor = 'blue', alpha = alpha) \n",
    "    \n",
    "    if not identity==[]:\n",
    "      plt.plot(range(len(identity)), identity, '--', color = 'green', label='identity')\n",
    "\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a16b1N-kCMgW"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ot1bbPDBCMgW"
   },
   "source": [
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGppbBvf1Btk"
   },
   "source": [
    "## train-set visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZpBQIo-1Ez7"
   },
   "outputs": [],
   "source": [
    "nRow = 2\n",
    "nCol = 12\n",
    "index_data                  = np.arange(0, nRow * nCol) # show only first images\n",
    "originals_train, blurry_train = dataset_train[index_data]\n",
    "originals_train              = originals_train[0]\n",
    "blurry_train                 = blurry_train[0]\n",
    "\n",
    "prediction_train = model(blurry_train.unsqueeze(dim=1).to(device))\n",
    "\n",
    "print('[clean train images]')\n",
    "plot_data_grid(originals_train, index_data, nRow, nCol)     \n",
    "print('[blurry train images]')\n",
    "plot_data_grid(blurry_train, index_data, nRow, nCol)   \n",
    "print('[deblurred train images]')\n",
    "plot_data_grid(prediction_train, index_data, nRow, nCol) \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wy4QU_Q404wd"
   },
   "source": [
    "## test-set visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sodCNtUoyRlI"
   },
   "outputs": [],
   "source": [
    "nRow = 2\n",
    "nCol = 10\n",
    "index_data                  = np.arange(0, nRow * nCol) # show only first images\n",
    "originals_test, blurry_test = dataset_test[index_data]\n",
    "originals_test              = originals_test[0]\n",
    "blurry_test                 = blurry_test[0]\n",
    "\n",
    "prediction_test = model(blurry_test.unsqueeze(dim=1).to(device))\n",
    "\n",
    "print('[clean test images]')\n",
    "plot_data_grid(originals_test, index_data, nRow, nCol)     \n",
    "print('[blurry test images]')\n",
    "plot_data_grid(blurry_test, index_data, nRow, nCol)   \n",
    "print('[deblurred test images]')\n",
    "plot_data_grid(prediction_test, index_data, nRow, nCol)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dj5LRQYM8OKI"
   },
   "source": [
    "## learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZLeTnTF9CMgd"
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "plot_curve_error(loss_mean_train, loss_std_train, loss_mean_test, loss_std_test, 'epoch', 'losses', 'LOSS')\n",
    "# accuracy - PSNR\n",
    "plot_curve_error(psnr_mean_train, psnr_std_train, psnr_mean_test, psnr_std_test, 'epoch', 'accuracy', 'PSNR')\n",
    "# accuracy - SSIM\n",
    "plot_curve_error(ssim_mean_train, ssim_std_train, ssim_mean_test, ssim_std_test, 'epoch', 'accuracy', 'SSIM')\n",
    "\n",
    "# notice that the 'train' signals were computed each batch while the test signals are computed at the end of the epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pEnNOZQNyoe9"
   },
   "source": [
    "## save your model (and submit as a separate file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uQIRyQ2ysyl"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mdXLZfM1I0u1"
   },
   "source": [
    "## evaluate on (public) test set (make sure that your model can load properly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w3-475PQy5Kt"
   },
   "outputs": [],
   "source": [
    "model = Network().to(device)\n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "(loss_test, psnr_test, ssim_test) = test(model, dataloader_test)\n",
    "print('Test PSNR: ', psnr_test['mean'])\n",
    "print('Test SSIM: ', ssim_test['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-B5k8QBfJYO"
   },
   "source": [
    "## Question: Are the outputs better than the inputs?\n",
    "Compute in the block below the PSNR and SSIM on the test set when using the input (blurry) images as the prediction. Compare to your results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_RcaQ52fvrO"
   },
   "outputs": [],
   "source": [
    "model = Network(dim_feature=8).to(device)\n",
    "model.load_state_dict(torch.load('./model.pth'))\n",
    "\n",
    "# Compute here the PSNR and SSIM of the blurry inputs: input_psnr_test and input_ssim_test\n",
    "\n",
    "# Then, uncomment these two rows:\n",
    "# print('Test PSNR: ', input_psnr_test['mean'])\n",
    "# print('Test SSIM: ', input_ssim_test['mean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XWRUCHyDdy5A"
   },
   "source": [
    "### Short (bulleted) description of your choices: (double-click to edit)\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n",
    "\n",
    "5.\n",
    "\n",
    "6.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ex3_2022_part2.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "17ed1555cfbb96ddcf655400d6c25a9cebe961c1b69daf25bae91d698acdd2a7"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
